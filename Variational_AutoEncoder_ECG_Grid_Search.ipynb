{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = '/media/nikhil/Data/Experiments/ECG Problem/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(category):\n",
    "    content = []\n",
    "\n",
    "    with open(base_dir+'sample rr interval data/line_count_rr_with_category.txt', 'r') as fr:\n",
    "        rr_name = fr.readlines()[0].split('\\r')[:-2]\n",
    "        for names in rr_name:\n",
    "            name = names.split(' ')\n",
    "            if name[2] ==category:\n",
    "                with open(base_dir+'sample rr interval data/'+name[1], 'r') as rr:\n",
    "                    l = rr.readlines()\n",
    "                    #for i, x in enumerate(l):\n",
    "                    l = [x.replace('\\tN\\n', '') for x in l]\n",
    "                    #print l\n",
    "                    content.append(l)\n",
    "    return content\n",
    "\n",
    "#Saving data in normal and abnormal since it will be \n",
    "#used again and again instead of calling function again and again\n",
    "normal = read_data(\"Normal\")\n",
    "abnormal = read_data(\"Abnormal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data(arr, length, window_size):\n",
    "    #generate multiple array of fixed length from a list\n",
    "    #taking window size of 10, can make it variable also\n",
    "    result = []\n",
    "    for i in xrange(0, len(arr), window_size):\n",
    "        #print (i)\n",
    "        if (i+length)>=len(arr):\n",
    "            break\n",
    "        result.append(arr[i:i+length])\n",
    "    #for res in result:\n",
    "    #    print (len(res))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function calls generate data to agument data and store\n",
    "#list of lists in new_content\n",
    "\n",
    "def prepare_data(content,length, window_size):\n",
    "    new_content = []\n",
    "    #np.empty([len(content),20])\n",
    "    j = 0\n",
    "    for i, con in enumerate(content):\n",
    "        if len(con) > (length-1):\n",
    "            result = generate_data(con, length, window_size)\n",
    "            for res in result:\n",
    "                new_content.append(res)\n",
    "                #j = j+1\n",
    "    return new_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def abnormalDataList(input_dim, window_size):\n",
    "    list_test_abnormal = []\n",
    "    test_abnormal = abnormal\n",
    "    #print(\"len1\", len(test_abnormal))\n",
    "    for test_ab in test_abnormal:\n",
    "        if(len(test_ab)>input_dim):\n",
    "            ab_list = []\n",
    "            ab_list.extend(generate_data(test_ab, input_dim, window_size))\n",
    "            #print (len(test_ab[-input_dim[input_dim_index]-1:-1]))\n",
    "            ab_list.append(test_ab[-input_dim-1:-1])\n",
    "            #print (\"Hello 2\", len(ab_list))\n",
    "            list_test_abnormal.append(ab_list)\n",
    "    return list_test_abnormal\n",
    "\n",
    "\n",
    "def normalDataList(input_dim, window_size):\n",
    "    content = normal\n",
    "    new_content = prepare_data(content, input_dim, window_size)\n",
    "    normal_data_length = len(new_content)\n",
    "    train_test_split = 0.85\n",
    "    test_normal = new_content[int(train_test_split*normal_data_length):-1]\n",
    "    new_content = new_content[:int(train_test_split*normal_data_length)]\n",
    "    \n",
    "    return new_content, test_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalAcc(thresh):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for res in norm_res:\n",
    "        if(res>=thresh):\n",
    "            incorrect =incorrect+1\n",
    "        else:\n",
    "            correct =correct+1\n",
    "    acc = correct/(correct+incorrect)\n",
    "    return correct, incorrect, acc\n",
    "#normalAcc()\n",
    "\n",
    "def abnormalAcc1(thresh):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    flag = 0\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for abnorm_res in list_abnorm_res:\n",
    "        for res in abnorm_res:\n",
    "            if(res>=thresh):\n",
    "                correct =correct+1\n",
    "                flag=1\n",
    "                break\n",
    "        if (flag==0):\n",
    "            incorrect =incorrect+1\n",
    "        flag=0\n",
    "    acc = correct/(correct+incorrect)        \n",
    "    return correct, incorrect, acc\n",
    "    \n",
    "#abnormalAcc1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "#This function is used in providing batch to tensorflow\n",
    "#model defined below. It return RR data of size batch size\n",
    "\n",
    "def next_batch(batch_size):\n",
    "    i = random.randint(0,len(new_content)-batch_size-1)\n",
    "    return new_content[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functiont to initialize weights\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "#helper fucntion to initialize bias\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_graph(input_dim):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "    #print (\"shape: \", x.get_shape())\n",
    "    \n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "    #Weight and Bais of input encoder\n",
    "    W_encoder_input_hidden = weight_variable(\n",
    "        [input_dim,hidden_encoder_dim])\n",
    "    b_encoder_input_hidden = bias_variable([hidden_encoder_dim])\n",
    "    \n",
    "    l2_loss += tf.nn.l2_loss(W_encoder_input_hidden)\n",
    "    \n",
    "    # Hidden layer encoder\n",
    "    hidden_encoder = tf.nn.relu(tf.matmul(\n",
    "            x, W_encoder_input_hidden) + b_encoder_input_hidden)\n",
    "    \n",
    "    #Weight and Bais of hidden mu\n",
    "    W_encoder_hidden_mu = weight_variable(\n",
    "        [hidden_encoder_dim,latent_dim])\n",
    "    b_encoder_hidden_mu = bias_variable([latent_dim])\n",
    "    \n",
    "    l2_loss += tf.nn.l2_loss(W_encoder_hidden_mu)\n",
    "    \n",
    "    # Mu encoder\n",
    "    mu_encoder = tf.matmul(\n",
    "        hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu\n",
    "    \n",
    "    #Weight and Bais of hidden logvar\n",
    "    W_encoder_hidden_logvar = weight_variable(\n",
    "        [hidden_encoder_dim,latent_dim])\n",
    "    b_encoder_hidden_logvar = bias_variable([latent_dim])\n",
    "    \n",
    "    l2_loss += tf.nn.l2_loss(W_encoder_hidden_logvar)\n",
    "    \n",
    "    # Sigma encoder\n",
    "    logvar_encoder = tf.matmul(\n",
    "        hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar\n",
    "    \n",
    "    # Sample epsilon using logvar_encoder\n",
    "    epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')\n",
    "    \n",
    "    # Sample latent variable\n",
    "    std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "    z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "    \n",
    "    #Weight and Bais of z hidden\n",
    "    W_decoder_z_hidden = weight_variable([latent_dim,hidden_decoder_dim])\n",
    "    b_decoder_z_hidden = bias_variable([hidden_decoder_dim])\n",
    "    \n",
    "    l2_loss += tf.nn.l2_loss(W_decoder_z_hidden)\n",
    "    \n",
    "    # Hidden layer decoder\n",
    "    hidden_decoder = tf.nn.relu(tf.matmul(\n",
    "            z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "    \n",
    "    #Weight and Bais of reconstruction\n",
    "    W_decoder_hidden_reconstruction = weight_variable(\n",
    "        [hidden_decoder_dim, input_dim])\n",
    "    b_decoder_hidden_reconstruction = bias_variable([input_dim])\n",
    "    \n",
    "    l2_loss += tf.nn.l2_loss(W_decoder_hidden_reconstruction)\n",
    "    \n",
    "    #KL Divergence\n",
    "    KLD = -0.5 * tf.reduce_sum(1 +\n",
    "                               logvar_encoder - \n",
    "                               tf.pow(mu_encoder, 2)-\n",
    "                               tf.exp(logvar_encoder),\n",
    "                               reduction_indices=1)\n",
    "\n",
    "    x_hat = tf.matmul(\n",
    "        hidden_decoder, \n",
    "        W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "\n",
    "    #BCE loss\n",
    "    BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=x_hat, labels=x), reduction_indices=1)\n",
    "\n",
    "    loss = tf.reduce_mean(BCE + KLD)\n",
    "\n",
    "    regularized_loss = loss + lam * l2_loss\n",
    "\n",
    "    loss_summ = tf.summary.scalar(\"lowerbound\", loss)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(regularized_loss)\n",
    "    \n",
    "    # add op for merging summary\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # add Saver ops\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter('experiment',\n",
    "                                               graph=sess.graph)\n",
    "        if os.path.isfile(base_dir+'VAE-TensorFlow-master/save/model.ckpt'):\n",
    "            print(\"Restoring saved parameters\")\n",
    "            saver.restore(\n",
    "                sess, base_dir+'VAE-TensorFlow-mastersave/model.ckpt')\n",
    "        else:\n",
    "            print(\"Initializing parameters\")\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for step in range(1, n_steps):\n",
    "            batch = next_batch(batch_size)\n",
    "\n",
    "            feed_dict = {x: batch}\n",
    "            _, cur_loss, summary_str = sess.run(\n",
    "                [train_step, regularized_loss, summary_op], feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            if step ==1:\n",
    "                max_threshold = min_threshold = cur_loss\n",
    "\n",
    "            if step % 500 == 0:\n",
    "                save_path = saver.save(\n",
    "                    sess, base_dir+\"VAE-TensorFlow-master/save/model.ckpt\")\n",
    "                #print(\"Step {0} | Loss: {1}\".format(step, cur_loss))\n",
    "                \n",
    "                if max_threshold < cur_loss:\n",
    "                    max_threshold = cur_loss\n",
    "                if min_threshold > cur_loss:\n",
    "                    min_threshold = cur_loss\n",
    "                \n",
    "            \n",
    "                \n",
    "        #start_threshold = cur_loss\n",
    "        norm_res = []\n",
    "        #abnorm_res = []\n",
    "        list_abnorm_res = []\n",
    "        for dat in test_normal:\n",
    "            norm_res.append(sess.run(regularized_loss, feed_dict={x:[dat]}))\n",
    "        #for dat in test_abnormal:\n",
    "        #    abnorm_res.append(sess.run(loss, feed_dict={x:[dat]}))\n",
    "        for test_abnormal in list_test_abnormal:\n",
    "            abnorm_res = []\n",
    "            for dat in test_abnormal:\n",
    "                #print (len(dat))\n",
    "                abnorm_res.append(sess.run(regularized_loss, feed_dict={x:[dat]}))\n",
    "            list_abnorm_res.append(abnorm_res)\n",
    "            \n",
    "    return norm_res, list_abnorm_res, min_threshold, max_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    print(\"Window Size \", window_size)\\n    print(\"index_length \", input_dim)\\n    print(\"Threshold\", threshold)\\n    print(\"Normal Acc\")\\n    \\n    print(acc1)\\n    print(\"Abnormal Acc\")\\n    print(acc2)\\n    print((acc1+acc2)/2)\\n    \\n    '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def result(thresh, window_size, input_dim):\n",
    "    _, _, acc1 = normalAcc(thresh)\n",
    "    _, _, acc2 = abnormalAcc1(thresh)\n",
    "    '''\n",
    "    print(\"Window| index_len|Thresh| Normal Acc | Abnormal Acc |  Score\")\n",
    "    print(window_size, \"       \",input_dim, \"    \", \"{0:.2f}\".format(thresh), \"  \",\n",
    "          \"{0:.5f}\".format(acc1),\" \",\"{0:.5f}\".format(acc2),\" \",\n",
    "          \"{0:.5f}\".format(((acc1+acc2)/2)))\n",
    "          \n",
    "    '''\n",
    "    acc3 = ((acc1+acc2)/2)\n",
    "    \n",
    "    return acc1, acc2, acc3\n",
    "    \n",
    "                    \n",
    "                \n",
    "'''\n",
    "    print(\"Window Size \", window_size)\n",
    "    print(\"index_length \", input_dim)\n",
    "    print(\"Threshold\", threshold)\n",
    "    print(\"Normal Acc\")\n",
    "    \n",
    "    print(acc1)\n",
    "    print(\"Abnormal Acc\")\n",
    "    print(acc2)\n",
    "    print((acc1+acc2)/2)\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parameters\n"
     ]
    }
   ],
   "source": [
    "n_steps = int(1e4)\n",
    "batch_size = 100\n",
    "\n",
    "#Hyperparameters \n",
    "grid_hidden_encoder_dim = [50, 75, 100, 150, 200]\n",
    "grid_hidden_decoder_dim = [50, 75, 100, 150, 200]\n",
    "grid_latent_dim = [5, 10,15, 20]\n",
    "grid_lam = [0.01, 0.02, 0.03, 0.04, 0.05, 0.07, 0.1]\n",
    "grid_learning_rate = [0.00001,0.00003, 0.0001, 0.0003]\n",
    "grid_window_size = [5, 10, 15,20]\n",
    "grid_input_dim = [10, 20, 30, 40, 50, 60, 70]\n",
    "\n",
    "\n",
    "#hidden_encoder_dim = 50\n",
    "#hidden_decoder_dim = 50\n",
    "#latent_dim = 10\n",
    "#lam = 0.02\n",
    "#learning_rate = 0.00001\n",
    "\n",
    "#preparing data of length of interval 20,\n",
    "#This function will agument that RR dataset by creating dataset of \n",
    "#fixed length using a window size of 10\n",
    "#window_size = 10\n",
    "\n",
    "#index of internal interval(input_dim)\n",
    "#input_dim_index = 2\n",
    "#input_dim = [20, 30, 50, 70]\n",
    "#threshold = 29.3\n",
    "\n",
    "\n",
    "index = ['grid_input_dim', 'grid_window_size', 'grid_hidden_encoder_dim', \n",
    "         'grid_hidden_decoder_dim', 'grid_latent_dim', 'grid_lam', \n",
    "         'grid_learning_rate', 'Threshold', 'Normal Accuracy',\n",
    "         'Abnormal Accuracy', 'Score']\n",
    "results = []\n",
    "\n",
    "\n",
    "for input_dim in grid_input_dim:\n",
    "    for window_size in grid_window_size:\n",
    "        for hidden_encoder_dim in grid_hidden_encoder_dim:\n",
    "            hidden_decoder_dim = hidden_encoder_dim\n",
    "            for latent_dim in grid_latent_dim:\n",
    "                for lam in grid_lam:\n",
    "                    for learning_rate in grid_learning_rate:\n",
    "                        new_content, test_normal = normalDataList(input_dim, window_size)\n",
    "                        #print (\"normal train len \", len(new_content))\n",
    "                        #print(\"normal test len\", len(test_normal))\n",
    "\n",
    "                        list_test_abnormal = abnormalDataList(input_dim, window_size)\n",
    "                        #print (\"abnormal test len\", len(list_test_abnormal))\n",
    "\n",
    "                        norm_res, list_abnorm_res, start_threshold, end_threshold = tf_graph(input_dim)\n",
    "                        #print(\"Start Threshold \", start_threshold, \"end_threshold \", end_threshold)\n",
    "                        \n",
    "                        #end_threshold = float(end_threshold)\n",
    "                        for thresh in np.arange(start_threshold, end_threshold, 0.2):\n",
    "                            acc1, acc2, acc3 = result(thresh, window_size, input_dim)\n",
    "                            results.append([input_dim, window_size, hidden_encoder_dim, hidden_decoder_dim, \n",
    "                                            latent_dim, lam, learning_rate, thresh, acc1, acc2, acc3])\n",
    "                            #print (results)\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(len(abnorm_res))\n",
    "print(len(norm_res))\n",
    "print(len(list_abnorm_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/media/nikhil/Data/Experiments/ECG Problem/Code/result/outfile', 'wb') as fp:\n",
    "    pickle.dump(results, fp)\n",
    "    \n",
    "    \n",
    "with open ('/media/nikhil/Data/Experiments/ECG Problem/Code/result/outfile', 'rb') as fp:\n",
    "    results = pickle.load(fp)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tfone]",
   "language": "python",
   "name": "conda-env-tfone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
