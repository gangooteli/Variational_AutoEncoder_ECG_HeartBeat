{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    content = []\n",
    "    #test_content = []\n",
    "    #for i in range(len(interval)):\n",
    "    #   content.append([])\n",
    "    #content.append([])\n",
    "\n",
    "    with open('/media/nikhil/Data/Experiments/ECG Problem/sample rr interval data/line_count_rr_with_category.txt', 'r') as fr:\n",
    "        rr_name = fr.readlines()[0].split('\\r')[:-2]\n",
    "        for names in rr_name:\n",
    "            name = names.split(' ')\n",
    "            #for nam in name:\n",
    "            #    print nam\n",
    "            #s = f.rstrip().replace('\\r', ' ').split(' ')\n",
    "            #print s\n",
    "            #ll=[]\n",
    "            if name[2] ==\"Normal\":\n",
    "                with open('/media/nikhil/Data/Experiments/ECG Problem/sample rr interval data/'+name[1], 'r') as rr:\n",
    "                    l = rr.readlines()\n",
    "                    #for i, x in enumerate(l):\n",
    "                    l = [x.replace('\\tN\\n', '') for x in l]\n",
    "                    #print l\n",
    "                    content.append(l)\n",
    "            #elif name[2] ==\"Abnormal\":\n",
    "            #    with open('/media/nikhil/Data/Experiments/ECG Problem/sample rr interval data/'+name[1], 'r') as rr:\n",
    "            #        l = rr.readlines()\n",
    "                    #for i, x in enumerate(l):\n",
    "            #        l = [x.replace('\\tN\\n', '') for x in l]\n",
    "                    #print l\n",
    "            #        test_content.append(l)\n",
    "\n",
    "                    #l = [x.rstrip() for x in l]\n",
    "            #for i, j in enumerate(interval):\n",
    "            #    if len(l) >= j:\n",
    "            #        content[i].append(l)\n",
    "    #for con in content:\n",
    "    #    print (len(con))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data(arr, length):\n",
    "    #generate multiple array of fixed length from a list\n",
    "    #taking window size of 10, can make it variable also\n",
    "    result = []\n",
    "    for i in xrange(0, len(arr), 10):\n",
    "        #print (i)\n",
    "        if (i+length)>=len(arr):\n",
    "            break\n",
    "        result.append(arr[i:i+length])\n",
    "    #for res in result:\n",
    "    #    print (len(res))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(content,length):\n",
    "    new_content = []\n",
    "    #np.empty([len(content),20])\n",
    "    j = 0\n",
    "    for i, con in enumerate(content):\n",
    "        if len(con) > (length-1):\n",
    "            result = generate_data(con, length)\n",
    "            for res in result:\n",
    "                new_content.append(res)\n",
    "                #j = j+1\n",
    "    return new_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1852\n"
     ]
    }
   ],
   "source": [
    "#preparing data of length of interval 20,\n",
    "\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#mnist = input_data.read_data_sets('MNIST')\n",
    "\n",
    "#set of intervals to try out autoencoder\n",
    "#input_dim = 784\n",
    "\n",
    "\n",
    "#index of internal interval(input_dim)\n",
    "i =0\n",
    "\n",
    "input_dim = [20, 30, 50, 70]\n",
    "content = read_data()\n",
    "new_content = prepare_data(content, input_dim[i])\n",
    "#abnormal_content = prepare_data(test_content, input_dim[i])\n",
    "#print (len(new_content))\n",
    "test_normal = new_content[10001:-1]\n",
    "new_content = new_content[:10000]\n",
    "print (len(new_content))\n",
    "print(len(test_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "iter = 0\n",
    "def next_batch(batch_size):\n",
    "    i = random.randint(0,len(new_content)-batch_size-1)\n",
    "    return new_content[i:i+batch_size]\n",
    "    #if((iter+batch_size)>=len(new_content)):\n",
    "    #    return[0:batch_size]\n",
    "    #    #iter =+ batch_size\n",
    "    #    con = new_content[iter:iter+batch_size]\n",
    "    #else:\n",
    "    #    iter=batch_size\n",
    "    #    return new_content[]\n",
    "    #return con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con = next_batch(10)\n",
    "#print (con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_encoder_dim = 200\n",
    "hidden_decoder_dim = 200\n",
    "latent_dim = 10\n",
    "lam = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (?, 20)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, input_dim[i]])\n",
    "print (\"shape: \", x.get_shape())\n",
    "l2_loss = tf.constant(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_encoder_input_hidden = weight_variable([input_dim[i],hidden_encoder_dim])\n",
    "b_encoder_input_hidden = bias_variable([hidden_encoder_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_encoder_input_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden layer encoder\n",
    "hidden_encoder = tf.nn.relu(tf.matmul(x, W_encoder_input_hidden) + b_encoder_input_hidden)\n",
    "\n",
    "W_encoder_hidden_mu = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "b_encoder_hidden_mu = bias_variable([latent_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_encoder_hidden_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mu encoder\n",
    "mu_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_mu) + b_encoder_hidden_mu\n",
    "\n",
    "W_encoder_hidden_logvar = weight_variable([hidden_encoder_dim,latent_dim])\n",
    "b_encoder_hidden_logvar = bias_variable([latent_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_encoder_hidden_logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigma encoder\n",
    "logvar_encoder = tf.matmul(hidden_encoder, W_encoder_hidden_logvar) + b_encoder_hidden_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample epsilon\n",
    "epsilon = tf.random_normal(tf.shape(logvar_encoder), name='epsilon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample latent variable\n",
    "std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "\n",
    "W_decoder_z_hidden = weight_variable([latent_dim,hidden_decoder_dim])\n",
    "b_decoder_z_hidden = bias_variable([hidden_decoder_dim])\n",
    "l2_loss += tf.nn.l2_loss(W_decoder_z_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden layer decoder\n",
    "hidden_decoder = tf.nn.relu(tf.matmul(z, W_decoder_z_hidden) + b_decoder_z_hidden)\n",
    "\n",
    "W_decoder_hidden_reconstruction = weight_variable([hidden_decoder_dim, input_dim[i]])\n",
    "b_decoder_hidden_reconstruction = bias_variable([input_dim[i]])\n",
    "l2_loss += tf.nn.l2_loss(W_decoder_hidden_reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KLD = -0.5 * tf.reduce_sum(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "\n",
    "x_hat = tf.matmul(hidden_decoder, W_decoder_hidden_reconstruction) + b_decoder_hidden_reconstruction\n",
    "BCE = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=x), reduction_indices=1)\n",
    "\n",
    "loss = tf.reduce_mean(BCE + KLD)\n",
    "\n",
    "regularized_loss = loss + lam * l2_loss\n",
    "\n",
    "loss_summ = tf.summary.scalar(\"lowerbound\", loss)\n",
    "train_step = tf.train.AdamOptimizer(0.00001).minimize(regularized_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add op for merging summary\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "# add Saver ops\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_steps = int(1e4)\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "abnormal_list = []\n",
    "with open('/media/nikhil/Data/Experiments/ECG Problem/sample rr interval data/line_count_rr_with_category.txt', 'r') as fr:\n",
    "    rr_name = fr.readlines()[0].split('\\r')[:-2]\n",
    "    #print len(rr_name)\n",
    "    for names in rr_name:\n",
    "        name = names.split(' ')\n",
    "        if(name[2]==\"Abnormal\" and int(name[0])<25 and int(name[0])>19):\n",
    "            abnormal_list.append(name)\n",
    "print(len(abnormal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_abnormal = []\n",
    "for i, name in enumerate(abnormal_list):\n",
    "    #print(name)\n",
    "    with open('/media/nikhil/Data/Experiments/ECG Problem/sample rr interval data/'+name[1], 'r') as rr:\n",
    "        l = rr.readlines()\n",
    "        #for i, x in enumerate(l):\n",
    "        l = [a.replace('\\tN\\n', '') for a in l]\n",
    "        #print (l)\n",
    "        test_abnormal.append(l)\n",
    "    #abnormal_list[i] = ab_list[1:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for test_ab in test_abnormal:\n",
    "    #print (len(test_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, test_ab in enumerate(test_abnormal):\n",
    "    test_abnormal[i] = test_ab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for test_ab in test_abnormal:\n",
    "    #print (len(test_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = []\n",
    "test_label = []\n",
    "test_set = test_normal + test_abnormal\n",
    "test_label = [1]* len(test_normal) + [0]*len(test_abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1884 1884\n"
     ]
    }
   ],
   "source": [
    "print (len(test_set), len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parameters\n",
      "Step 500 | Loss: 13.7541122437\n",
      "Step 1000 | Loss: 13.545083046\n",
      "Step 1500 | Loss: 13.2571487427\n",
      "Step 2000 | Loss: 12.9105873108\n",
      "Step 2500 | Loss: 12.5100793839\n",
      "Step 3000 | Loss: 12.3615627289\n",
      "Step 3500 | Loss: 12.2945365906\n",
      "Step 4000 | Loss: 12.5148267746\n",
      "Step 4500 | Loss: 12.1308040619\n",
      "Step 5000 | Loss: 12.693312645\n",
      "Step 5500 | Loss: 12.0191879272\n",
      "Step 6000 | Loss: 12.274471283\n",
      "Step 6500 | Loss: 11.9967470169\n",
      "Step 7000 | Loss: 12.461974144\n",
      "Step 7500 | Loss: 11.9551830292\n",
      "Step 8000 | Loss: 11.6970796585\n",
      "Step 8500 | Loss: 11.7845067978\n",
      "Step 9000 | Loss: 12.2278928757\n",
      "Step 9500 | Loss: 12.1853275299\n",
      "26\n",
      "6\n",
      "0.8125\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    summary_writer = tf.summary.FileWriter('experiment',\n",
    "                                           graph=sess.graph)\n",
    "    if os.path.isfile(\"/media/nikhil/Data/Experiments/ECG Problem/VAE-TensorFlow-master/save/model.ckpt\"):\n",
    "        print(\"Restoring saved parameters\")\n",
    "        saver.restore(sess, \"/media/nikhil/Data/Experiments/ECG Problem/VAE-TensorFlow-mastersave/model.ckpt\")\n",
    "    else:\n",
    "        print(\"Initializing parameters\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for step in range(1, n_steps):\n",
    "        #y = np.hstack(content[3])\n",
    "        #print(y.shape)\n",
    "        batch = next_batch(batch_size)\n",
    "        #print (batch)\n",
    "        #batch = np.array([np.array(con) for con in content[3] if len(con)==70])\n",
    "        #for ba in batch:\n",
    "        # print(\"Shape: \", ba.shape, \"  \", ba)\n",
    "        #print(batch.shape)\n",
    "        #assert all(x.shape == batch for x in batch)\n",
    "        feed_dict = {x: batch}\n",
    "        #print (feed_dict)\n",
    "        _, cur_loss, summary_str = sess.run([train_step, loss, summary_op], feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            save_path = saver.save(sess, \"/media/nikhil/Data/Experiments/ECG Problem/VAE-TensorFlow-master/save/model.ckpt\")\n",
    "            print(\"Step {0} | Loss: {1}\".format(step, cur_loss))\n",
    "    \n",
    "    norm_res = []\n",
    "    abnorm_res = []\n",
    "    for dat in test_normal:\n",
    "        norm_res.append(sess.run(loss, feed_dict={x:[dat]}))\n",
    "    for dat in test_abnormal:\n",
    "        abnorm_res.append(sess.run(loss, feed_dict={x:[dat]}))\n",
    "        \n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    #for res in norm_res:\n",
    "        #if(res>12.5):\n",
    "        #    incorrect =+1\n",
    "        #else:\n",
    "        #    correct =+1 \n",
    "    for res in abnorm_res:\n",
    "        if(res>12.5):\n",
    "            correct =correct+1\n",
    "        else:\n",
    "            incorrect =incorrect+1\n",
    "            \n",
    "    acc = correct/(correct+incorrect)\n",
    "    print (correct)\n",
    "    print (incorrect)\n",
    "    print (acc)\n",
    "    #feed_dict = {x:test_abnormal}\n",
    "    #abnormal_error = sess.run(loss, feed_dict={x:test_abnormal})\n",
    "    #print (\"abnormal error: \", abnormal_error)\n",
    "    #normal_error = sess.run(loss, feed_dict={x:test_normal})\n",
    "    #print (\"normal error: \", normal_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4262\n",
      "13.1899\n",
      "13.2622\n",
      "14.0756\n",
      "14.1074\n",
      "13.8549\n",
      "13.1181\n",
      "13.9763\n",
      "12.9841\n",
      "13.1739\n",
      "13.9356\n",
      "13.6769\n",
      "13.3901\n",
      "14.0208\n",
      "13.1151\n",
      "11.3862\n",
      "12.4156\n",
      "12.9509\n",
      "13.1185\n",
      "15.323\n",
      "14.1899\n",
      "13.2342\n",
      "12.2341\n",
      "12.8121\n",
      "12.6187\n",
      "13.093\n",
      "12.0012\n",
      "11.4237\n",
      "15.2129\n",
      "12.1326\n",
      "13.4211\n",
      "16.2268\n"
     ]
    }
   ],
   "source": [
    "for res in abnorm_res:\n",
    "    print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "1852\n"
     ]
    }
   ],
   "source": [
    "print(len(abnorm_res))\n",
    "print(len(norm_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1525\n",
      "327\n",
      "0.82343412527\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "for res in norm_res:\n",
    "        if(res>=13):\n",
    "            incorrect =incorrect+1\n",
    "        else:\n",
    "            correct =correct+1\n",
    "acc = correct/(correct+incorrect)        \n",
    "print (correct)\n",
    "print (incorrect)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "10\n",
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "for res in abnorm_res:\n",
    "        if(res<13):\n",
    "            incorrect =incorrect+1\n",
    "        else:\n",
    "            correct =correct+1\n",
    "acc = correct/(correct+incorrect)        \n",
    "print (correct)\n",
    "print (incorrect)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tfone]",
   "language": "python",
   "name": "conda-env-tfone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
